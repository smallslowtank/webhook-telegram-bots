import asyncio
import json
import os

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–ª–µ–≥—Ä–∞–º–º
from telegram import Update, Message, Chat
from telegram.ext import filters, MessageHandler, ApplicationBuilder, CommandHandler, ContextTypes

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è GPT –∏ Langchain
from yandex_chain import YandexLLM
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Default settings
default_temperature = 0.3
default_context_window = 8000
model_list = [
    "GPT –ë–∞–∑–æ–≤–∞—è",
    "GPT –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è"
]
default_model_index = 1  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à—É—é –º–æ–¥–µ–ª—å GPT

# Current settings
current_temperature = default_temperature
current_context_window = default_context_window
current_model_index = default_model_index

HELP_MESSAGE = """–ö–æ–º–∞–Ω–¥—ã:
‚ö™ /mode ‚Äì –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å
‚ö™ /settings ‚Äì –ü–æ–∫–∞–∑–∞—Ç—å –∏ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
‚ö™ /help ‚Äì –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫—É

"""

SETTINGS_MESSAGE = """C—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:
‚ö™ –°—Ç–µ–ø–µ–Ω—å –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ /temperature = {current_temperature}. 
    –î–æ–ø—É—Å—Ç–∏–º—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: –æ—Ç 0 –¥–æ 1
‚ö™ –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ /context = {current_context_window}. 
    –î–æ–ø—É—Å—Ç–∏–º—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: –æ—Ç 1000 –¥–æ 8000
"""

ABOUT_MESSAGE = """
–ú–µ–Ω—è –∑–æ–≤—É—Ç –Ø—à–∞. 

–Ø –±–æ—Ç-–ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Å—Ç—Ä–æ–µ–Ω –Ω–∞ –±–∞–∑–µ GPT –º–æ–¥–µ–ª–µ–π.
–ú–æ—è –æ—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ - –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤—Å–µ —Ç–≤–æ–∏ –≤–æ–ø—Ä–æ—Å—ã, –∏ –ø–æ–º–æ–≥–∞—Ç—å –≤ —Ä–µ—à–µ–Ω–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á.

–¢–∏–ø–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Ä–∞–±–æ—Ç—ã —Å–æ –º–Ω–æ–π - —ç—Ç–æ —É–∑–Ω–∞–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏–ª–∏ —Ä–µ—à–∞—Ç—å –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏ :)
–ü–æ—Å—Ç–∞—Ä–∞—é—Å—å –±—ã—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–º!

–¢—ã –º–æ–∂–µ—à—å –≤—ã–±—Ä–∞—Ç—å GPT –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é —Ö–æ—á–µ—à—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å.
–ú–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Ä–∞–∑–º–µ—Ä –µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏. 
"""


async def show_chat_modes_handle(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global model_list, current_model_index
    current_model = model_list[current_model_index]
    message_local = """
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {model} –º–æ–¥–µ–ª—å
–ß—Ç–æ–±—ã –ø–æ–º–µ–Ω—è—Ç—å –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–π –∫–æ–º–∞–Ω–¥—ã:
    /model = lite     –¥–ª—è –±–∞–∑–æ–≤–æ–π GPT –º–æ–¥–µ–ª–∏
    /model = pro     –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π GPT –º–æ–¥–µ–ª–∏
    """
    formatted_message = message_local.format(model=current_model)
    reply_text = formatted_message
    await context.bot.send_message(chat_id=update.effective_chat.id, text=reply_text)
    # await update.message.reply_text(text, reply_markup=reply_markup, parse_mode=ParseMode.HTML)


async def show_settings_handle(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global current_temperature, current_context_window
    formatted_message = SETTINGS_MESSAGE.format(current_temperature=current_temperature,
                                                current_context_window=current_context_window)
    reply_text = formatted_message
    await context.bot.send_message(chat_id=update.effective_chat.id, text=reply_text)


async def show_help_handle(update: Update, context: ContextTypes.DEFAULT_TYPE):
    reply_text = ""
    reply_text += ABOUT_MESSAGE
    await context.bot.send_message(chat_id=update.effective_chat.id, text=reply_text)


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    reply_text = """
–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –Ø—à–∞. \n
–Ø –±–æ—Ç-–ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Å—Ç—Ä–æ–µ–Ω –Ω–∞ –±–∞–∑–µ GPT –º–æ–¥–µ–ª–µ–π.
–ú–æ—è –æ—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ - –ø–æ–º–æ—á—å —Ç–µ–±–µ —Å –æ—Ç–≤–µ—Ç–∞–º–∏ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ —Å —Ä–µ—à–µ–Ω–∏–µ–º –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á.
–¢–æ–ª—å–∫–æ –Ω–µ –∑–∞–±—É–¥—å –≤—ã—Å—Ç–∞–≤–∏—Ç—å –Ω—É–∂–Ω—É—é —Å—Ç–µ–ø–µ–Ω—å –º–æ–µ–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏!
ü§ñ
"""
    reply_text += HELP_MESSAGE

    await context.bot.send_message(chat_id=update.effective_chat.id, text=reply_text)


async def set_temperature_handle(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global current_temperature
    message_text = update.message.text
    if message_text.startswith('/temperature = '):
        value = message_text.split('=')[-1].strip()
    elif message_text.startswith('/temperature '):
        value = message_text.split(' ')[-1].strip()
    temperature = float(value)
    if 0 <= temperature <= 1:
        current_temperature = temperature
    message_local = """
    –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã —Ç–µ–ø–µ—Ä—å = {current_temperature}
    """
    formatted_message = message_local.format(current_temperature=current_temperature)
    reply_text = formatted_message
    await context.bot.send_message(chat_id=update.effective_chat.id, text=reply_text)


async def set_context_handle(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global current_context_window
    message_text = update.message.text
    if message_text.startswith('/context = '):
        value = message_text.split('=')[-1].strip()
    elif message_text.startswith('/context '):
        value = message_text.split(' ')[-1].strip()
    context = int(value)
    if 1000 <= context <= 8000:
        current_context_window = context
    message_local = """
    –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ —Ç–µ–ø–µ—Ä—å = {current_context} —Ç–æ–∫–µ–Ω–æ–≤.
    """
    formatted_message = message_local.format(current_context=current_context_window)
    reply_text = formatted_message
    await context.bot.send_message(chat_id=update.effective_chat.id, text=reply_text)


async def set_model_handle(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global model_list, current_model_index

    message_text = update.message.text
    if '/model = lite' in message_text or '/model lite' in message_text:
        current_model_index = 0
    elif '/model = pro' in message_text or '/model pro' in message_text:
        current_model_index = 1

    current_model = model_list[current_model_index]
    message_local = """
    –¢–µ–∫—É—â–∞—è LLM-–º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –∏—Å–ø–æ–ª—å–∑—É–µ–º: {current_model}.
    """
    formatted_message = message_local.format(current_model=current_model)
    reply_text = formatted_message
    await context.bot.send_message(chat_id=update.effective_chat.id, text=reply_text)


async def process_text_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global current_temperature, current_context_window, current_model_index
    # –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    message_text = update.message.text

    if message_text == "/start":
        # await context.bot.send_message(chat_id=update.effective_chat.id, text=update.message.text)
        await start(update, context)
    elif message_text == "/mode":
        await show_chat_modes_handle(update, context)
    elif message_text == "/settings":
        await show_settings_handle(update, context)
    elif message_text == "/help":
        await show_help_handle(update, context)
    elif message_text.startswith("/temperature"):
        await set_temperature_handle(update, context)
    elif message_text.startswith("/context"):
        await set_context_handle(update, context)
    elif message_text.startswith("/model"):
        await set_model_handle(update, context)
    else:
        # –æ–±—Ä–∞—â–µ–Ω–∏–µ –∫ –º–æ–¥–µ–ª–∏ GPT
        gpt_folder_id = os.environ["GPT_FOLDER_ID"]
        gpt_api_key = os.environ["GPT_API_KEY"]
        gpt_temperature = current_temperature
        gpt_max_tokens = current_context_window

        # if current_model_index==0:
        # model_uri = "gpt://"+str(yagpt_folder_id)+"/yandexgpt-lite/latest"
        # else:
        # model_uri = "gpt://"+str(yagpt_folder_id)+"/yandexgpt/latest"
        # model = ChatYandexGPT(api_key=yagpt_api_key, model_uri=model_uri, temperature = yagpt_temperature)

        gpt_prompt = """
        –¢–µ–±—è –∑–æ–≤—É—Ç –Ø—à–∞. –¢—ã –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∫–æ—Ä–æ—Ç–∫–æ –∏ —Ç–æ—á–Ω–æ. –°–ª–µ–¥—É–π –ø–æ–∂–µ–ª–∞–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –¥–æ —Ç–µ—Ö –ø–æ—Ä –ø–æ–∫–∞ –æ–Ω–∏ –Ω–µ –≤—ã—Ö–æ–¥—è—Ç –∑–∞ —Ä–∞–º–∫–∏ —ç—Ç–∏–∫–∏. –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç, –≤–µ–∂–ª–∏–≤–æ –Ω–∞–ø–∏—à–∏ –æ–± —ç—Ç–æ–º.
        """
        if current_model_index == 1: bool_lite = False  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –º–æ–¥–µ–ª—å GPT
        if current_model_index == 0: bool_lite = True  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å GPT
        llm = YandexLLM(api_key=gpt_api_key, folder_id=gpt_folder_id, temperature=gpt_temperature,
                        max_tokens=gpt_max_tokens, use_lite=bool_lite)
        str2 = """–í–æ–ø—Ä–æ—Å: {question}
        –¢–≤–æ–π –æ—Ç–≤–µ—Ç:"
        """
        template = f"{gpt_prompt} {str2}"
        prompt = PromptTemplate(template=template, input_variables=["question"])
        llm_chain = LLMChain(prompt=prompt, llm=llm)
        response = llm_chain.run({"question": message_text})
        await context.bot.send_message(chat_id=update.effective_chat.id, text=response)


bot = ApplicationBuilder().token(os.environ["BOT_TOKEN"]).updater(None).build()

bot.add_handler(CommandHandler("start", start))
bot.add_handler(CommandHandler("mode", show_chat_modes_handle))

bot.add_handler(MessageHandler(filters.TEXT & (~filters.COMMAND), process_text_message))

loop = asyncio.get_event_loop()
loop.run_until_complete(bot.initialize())


async def handler(event, context):
    message = json.loads(event["messages"][0]["details"]["message"]["body"])
    update = Update(
        update_id=message["update_id"],
        message=Message(
            message_id=message["message"]["message_id"],
            date=message["message"]["date"],
            chat=Chat(
                id=message["message"]["chat"]["id"],
                type=message["message"]["chat"]["type"]
            ),
            text=message["message"]["text"],
        )
    )

    await bot.process_update(update)

    return {
        "statusCode": 500,
    }
